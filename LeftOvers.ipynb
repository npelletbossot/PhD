{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Librairies --- #\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.float_info.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Fitting --- #\n",
    "\n",
    "\n",
    "# # Linear law : y=a*x+b\n",
    "# def linear_law(_time_, _a_):\n",
    "#     return _a_ * _time_ \n",
    "\n",
    "# # Power law : y=a*(x)**b\n",
    "# def power_law(_time_, _a_, _b_):\n",
    "#     return _a_ * (_time_**_b_)\n",
    "\n",
    "# # Logarithm law : y=a*ln(1+x)**b\n",
    "# def logarithm_law(_time_, a, b):\n",
    "#     return a * (np.log(_time_ + 1)**b)\n",
    "\n",
    "# # Power law modified : y=a*(x+c)**b\n",
    "# def modified_power_law(_time_, a, b, c):\n",
    "#     return a * (_time_ + c)**b  \n",
    "\n",
    "# # Sigmoid law : y=L/(1+e−k(x−x0​))\n",
    "# def sigmoid_law(_time_, L, k, x0):\n",
    "#     return L / (1 + np.exp(-k * (_time_ - x0)))\n",
    "\n",
    "# # Exponentially attenuated power law : y=a*x^b*e^−c*x\n",
    "# def damped_power_law(_time_, a, b, c):\n",
    "#     return a * _time_**b * np.exp(-c * _time_)\n",
    "\n",
    "# # Square root law : y=a*sqrt(x+c)\n",
    "# def sqrt_law(_time_, a, c):\n",
    "#     return a * np.sqrt(_time_ + c)\n",
    "\n",
    "# # Rational function law : y=(a*x+b)/(c+x)\n",
    "# def rational_function_law(_time_, a, b, c):\n",
    "#     return (a * _time_ + b) / (c + _time_)\n",
    "\n",
    "# # Polynomial logarithmic law : y=a*ln(x+c)+b*ln(x+c)^2+d\n",
    "# def poly_log_law(_time_, a, b, c, d):\n",
    "#     return a * np.log(_time_ + c) + b * (np.log(_time_ + c)**2) + d\n",
    "\n",
    "# # Generalized power law : y=a*(x^b+c)^d\n",
    "# def generalized_power_law(_time_, a, b, c, d):\n",
    "#     return a * ((_time_**b) + c)**d\n",
    "\n",
    "\n",
    "# # Ideas\n",
    "# def linear_and_power_law(_time_, _v_,  _D_, _alpha_, _tau_, _h_):\n",
    "#     return (((_v_*_time_)/(1+(_time_/_tau_)**_h_)) + (((_time_/_tau_)**_h_) /(1+(_time_/_tau_)**_h_)) * (_D_*_time_**_alpha_))\n",
    "\n",
    "# def linear_and_logarithmic_law(_time_, _v_,  _A_, _B_, _tau_, _h_):\n",
    "#     return (((_v_*_time_)/(1+(_time_/_tau_)**_h_)) + (((_time_/_tau_)**_h_) /(1+(_time_/_tau_)**_h_)) * _A_ * (np.log(_time_ + 1)**_B_))\n",
    "\n",
    "# def power_and_logarithmic_law(_time_, _D_, _alpha_, _A_, _B_, _tau_, _h_):\n",
    "#     return (((1/(1+(_time_/_tau_)**_h_)) * (_D_*_time_**_alpha_)) + (((_time_/_tau_)**_h_) /(1+(_time_/_tau_)**_h_)) * _A_ * (np.log(_time_ + 1)**_B_))\n",
    "\n",
    "\n",
    "# # List of laws\n",
    "# laws = {\n",
    "#     # \"Linear_law\":linear_law,\n",
    "#     \"Power law\": power_law,\n",
    "#     \"Logarithm law\":logarithm_law,\n",
    "#     \"Linear_and_power_law\":linear_and_power_law ,\n",
    "#     \"Linear_and_logarithmic_law\":linear_and_logarithmic_law,\n",
    "#     \"Power_and_logarithmic_law\":power_and_logarithmic_law\n",
    "# }\n",
    "\n",
    "\n",
    "# # Analysis for any law\n",
    "# def fitting_processus(law, _data_to_fit_, _time_list_):\n",
    "#     \"\"\"\n",
    "#     Fit the provided law to the given data\n",
    "\n",
    "#     Args:\n",
    "#         law (callable): The mathematical law to fit (inear_law, power_law, ...).\n",
    "#         _data_to_fit_ (list or np.array): The data points to fit.\n",
    "#         _time_list_ (list or np.array): The corresponding time values.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: Fitted parameters (rounded to 2 decimals).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Fit the provided law to the data\n",
    "#     # _fitting_parameters_, _fitting_errors_ = curve_fit(f=law, xdata=_time_list_, ydata=_data_to_fit_, maxfev=100000)\n",
    "#     _fitting_parameters_, _fitting_errors_ = curve_fit(f=law, xdata=_time_list_, ydata=_data_to_fit_, sigma=data_std, maxfev=1000000)\n",
    "#     # _fitting_parameters_ = np.round(_fitting_parameters_, 2)\n",
    "#     return _fitting_parameters_, _fitting_errors_\n",
    "\n",
    "\n",
    "# # Datas\n",
    "# data_to_fit = df_polars['mean_results'].to_list()[0]\n",
    "# data_std = df_polars['std_results'].to_list()[0]\n",
    "# time_step = 1\n",
    "# time_list = np.arange(0, len(data_to_fit), time_step)\n",
    "\n",
    "\n",
    "# # Filtering datas (essential)\n",
    "# valid_idx = ~np.isnan(data_to_fit) & ~np.isnan(data_std) & ~np.isinf(data_to_fit) & ~np.isinf(data_std)\n",
    "# data_to_fit = np.array(data_to_fit)[valid_idx]\n",
    "# data_std = np.array(data_std)[valid_idx]\n",
    "# time_list = np.array(time_list)[valid_idx]\n",
    "\n",
    "# # Correction on the sigma values (essential)\n",
    "# data_std = np.where(data_std == 0, 1e-5, data_std)\n",
    "\n",
    "\n",
    "# # Subplots configuration\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# plt.suptitle(f'k={k_w} - theta={theta_w} - scenario={alpha_w}')\n",
    "# axes[0].set_title(\"Échelle classique\")\n",
    "# axes[0].set_xlabel(\"Temps\")\n",
    "# axes[0].set_ylabel(\"Données\")\n",
    "# axes[1].set_title(\"Échelle log-log\")\n",
    "# axes[1].set_xlabel(\"Temps (log)\")\n",
    "# axes[1].set_ylabel(\"Données (log)\")\n",
    "# axes[1].set_xscale(\"log\")\n",
    "# axes[1].set_yscale(\"log\")\n",
    "# axes[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# # Plot original data\n",
    "# axes[0].errorbar(time_list, data_to_fit, yerr=data_std, fmt='o', color='gray', alpha=0.25, label=\"Datas\")\n",
    "# axes[1].errorbar(time_list, data_to_fit, yerr=data_std, fmt='o', color='gray', alpha=0.25, label=\"Datas\")\n",
    "\n",
    "# # Iterate over laws and fit each one\n",
    "# for name, law in laws.items():\n",
    "#     try:\n",
    "#         fitting_parameters, fitting_errors = fitting_processus(law, data_to_fit, time_list)\n",
    "#         param_errors = np.sqrt(np.diag(fitting_errors))\n",
    "\n",
    "#         fitted_data = law(time_list, *fitting_parameters)\n",
    "#         axes[0].scatter(time_list, fitted_data, label=f\"{name}\", marker='+', alpha=1)\n",
    "#         axes[1].scatter(time_list, fitted_data, label=f\"{name}\", marker='+', alpha=1)\n",
    "        \n",
    "#         print(f\"{name} fit successful with parameters: {fitting_parameters} and errors: {param_errors} \\n\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"{name} fit failed: {e}\")\n",
    "\n",
    "# # Add legends\n",
    "# axes[0].legend()\n",
    "# axes[1].legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_P(\n",
    "#     jump_array: np.ndarray, \n",
    "#     times: np.ndarray, \n",
    "#     Pzero: np.ndarray, \n",
    "#     lp: float, \n",
    "#     Rc_max: float, \n",
    "#     Rc_min: float, \n",
    "#     F_condensin: float,\n",
    "#     ryu_y: np.ndarray\n",
    "# ) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "#     \"\"\"\n",
    "#     Computes the probability distribution P using Pjump, calculates the equilibrium \n",
    "#     distribution Peq using Ploop_rosa, and performs binning and normalization.\n",
    "\n",
    "#     Args:\n",
    "#         jump_array (np.ndarray): Array of jump lengths.\n",
    "#         times (np.ndarray): Array of time points.\n",
    "#         Pzero (np.ndarray): Initial probability distribution.\n",
    "#         lp (float): Persistence length.\n",
    "#         Rc_max (float): Maximum condensin radius.\n",
    "#         Rc_min (float): Minimum condensin radius.\n",
    "#         F_condensin (float): Condensin force.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, float]: \n",
    "#             - P_binned (np.ndarray): Binned probability distribution.\n",
    "#             - Peq_binned (np.ndarray): Binned equilibrium distribution.\n",
    "#             - median_array (np.ndarray): Median values for P_binned.\n",
    "#             - x_P32 (np.ndarray): X values for the P^(-3/2) reference distribution.\n",
    "#             - y_P32 (np.ndarray): Y values for the P^(-3/2) reference distribution.\n",
    "#             - Peq (np.ndarray): Equilibrium probability distribution.\n",
    "#             - sum_ryu_y (float): Sum of Ryu reference data.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Compute the probability distribution P using Pjump\n",
    "#     P: np.ndarray = Pjump(\n",
    "#         lengths=jump_array, \n",
    "#         time=times, \n",
    "#         P0=Pzero, \n",
    "#         lp=lp, \n",
    "#         Rc=Rc_max, \n",
    "#         Rmin=Rc_min, \n",
    "#         f=F_condensin\n",
    "#     )\n",
    "\n",
    "#     # Transpose to have one subarray per time step and normalize each row\n",
    "#     P = np.transpose(P)\n",
    "#     P /= np.sum(P, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "#     # Compute the equilibrium distribution Peq using Ploop_rosa\n",
    "#     alphac = Rc_max / lp\n",
    "#     alphamin = Rc_min / lp\n",
    "#     kappa = lp / (jump_array + 1e-30)\n",
    "\n",
    "#     Peq: np.ndarray = np.array(\n",
    "#         Pool().map(\n",
    "#             Ploop_rosa,\n",
    "#             kappa,\n",
    "#             [alphac] * len(kappa),\n",
    "#             [alphamin] * len(kappa),\n",
    "#             [lp] * len(kappa),\n",
    "#             [F_condensin] * len(kappa),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # Normalizing\n",
    "#     Peq /= np.sum(Peq)\n",
    "\n",
    "\n",
    "#     # Compute Peq_binned and its median\n",
    "#     Peq_binned: np.ndarray = calculate_distrib_for_ryu(np.copy(Peq))\n",
    "#     Peq_binned /= np.sum(Peq_binned)\n",
    "#     Peq_med: int = find_median(Peq_binned)\n",
    "\n",
    "#     # Verify Peq normalization \n",
    "#     print(\"Sum Peq:\", np.sum(Peq))\n",
    "\n",
    "\n",
    "#     # Compute P_binned and its median\n",
    "#     P_binned: np.ndarray = np.array([calculate_distrib_for_ryu(row) for row in np.copy(P)], dtype=float)\n",
    "#     P_meds: np.ndarray = np.array([find_median(row) for row in np.copy(P_binned)], dtype=int)\n",
    "\n",
    "#     # Normalize P_binned (avoid division by zero)\n",
    "#     row_sums = np.sum(P_binned, axis=1, keepdims=True)\n",
    "#     row_sums[row_sums == 0] = 1\n",
    "#     P_binned /= row_sums\n",
    "\n",
    "#     # Verify normalization of P_binned\n",
    "#     print(\"Sum of each line of P_binned:\", np.sum(P_binned, axis=1))\n",
    "\n",
    "\n",
    "#     # Compute P^(3/2) reference function\n",
    "#     x_P32 = np.arange(1, jump_array.max(), 1)\n",
    "#     y_P32 = x_P32 ** (-3/2)\n",
    "\n",
    "#     # Verify sum of Ryu reference data\n",
    "#     sum_ryu_y = np.sum(ryu_y)\n",
    "#     print(\"Sum Ryu_y:\", sum_ryu_y)\n",
    "\n",
    "\n",
    "#     # End\n",
    "#     return Peq, Peq_binned, Peq_med, P, P_binned, P_meds, x_P32, y_P32\n",
    "\n",
    "\n",
    "\n",
    "# def plot_distribution(times, ryu_points, ryu_x, ryu_y, Peq_binned, Peq_med, P_binned, x_P32, y_P32, ax, log_scale=False):\n",
    "#     \"\"\" \n",
    "#     Plotting the distribution of : P32 + P_binned + Peq_binned. \n",
    "#     log_scale (bool): False for cartesian, True for log-log.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # P32\n",
    "#     ax.plot(x_P32, y_P32, color='grey', label='P32', marker='+', alpha=0.5)\n",
    "\n",
    "#     # P_binned\n",
    "#     for k in range(len(P_binned)):\n",
    "#         line, = ax.plot(ryu_points, P_binned[k], marker='+', label=f'P{k} (t={times[k]})')\n",
    "    \n",
    "#     # Peq_binned\n",
    "    \n",
    "#     ax.plot(ryu_points, Peq_binned, marker='+', color='black', label='Peq', ls=':')\n",
    "#     # ax.plot(ryu_points, Peq_binned, marker='+', color='black', label='Peq', ls=':')\n",
    "#     ax.axvline(x=Peq_med, color='black', ls='--', label=f'med = {Peq_med}')\n",
    "\n",
    "#     # Ryu datas\n",
    "#     ax.plot(ryu_x, ryu_y, c='k', lw=1, ls='-', marker='+', label='ryu_datas')\n",
    "\n",
    "#     # Scale    \n",
    "#     if log_scale:\n",
    "#         ax.set_xscale('log')\n",
    "#         ax.set_yscale('log')\n",
    "#         title = 'Distribution - Log-Log'\n",
    "#     else:\n",
    "#         title = 'Distribution - Cartesian'\n",
    "\n",
    "#     # Parameters\n",
    "#     ax.set_xlabel('x (bp)')\n",
    "#     ax.set_ylabel('p (binned and normalized)')\n",
    "#     ax.set_xlim([1e0, 2e3])\n",
    "#     ax.set_ylim([1e-10, 1e0])\n",
    "#     ax.grid(True, which='both')\n",
    "#     ax.set_title(title)\n",
    "#     ax.legend()\n",
    "\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.array([\n",
    "    [1,1,2,1,2,1],\n",
    "    [1,4,5,4,5,1],\n",
    "    [2,2,2,2,2,2]\n",
    "])\n",
    "\n",
    "print(matrix)\n",
    "\n",
    "new_matrix = matrix / np.sum(matrix, axis=1, keepdims=True)\n",
    "print(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "times = np.array([1, 10, 100, 1000])\n",
    "median_array = np.array([10,20,20,30])\n",
    "\n",
    "median_matrix = np.empty(len(times), dtype=object)\n",
    "print(median_matrix)\n",
    "\n",
    "for i in range(len(times)):\n",
    "    median_matrix[i] = np.array(median_array)\n",
    "\n",
    "print(median_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "times = np.array([1, 10, 100, 1000])\n",
    "median_array = np.array([10, 20, 20, 30])\n",
    "\n",
    "median_matrix = np.empty((len(times), len(median_array)), dtype=int)\n",
    "\n",
    "for i in range(len(times)):\n",
    "    median_matrix[i] = median_array\n",
    "\n",
    "print(median_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Exemple de numpy array\n",
    "arr = np.array([[0.000123456789, 12345.6789, 0.987654321], \n",
    "                [3.1415926535, 2.7182818284, 1.6180339887]])\n",
    "\n",
    "# Convertir en notation scientifique avec 4 chiffres significatifs\n",
    "scientific_arr = np.vectorize(lambda x: f\"{x:.4e}\")(arr)\n",
    "\n",
    "print(scientific_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "median_matrix = np.zeros((5, 3), dtype=int)\n",
    "print(median_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "all_x = np.array([0,2,4,7,25,34,51,60,61,67,70])\n",
    "Lmax= 100\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_x,marker='+')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "HiC_map = np.zeros((Lmax, Lmax))\n",
    "\n",
    "# # method 1\n",
    "# for x1, x2 in zip(all_x, all_x[1:]):\n",
    "#     print(x1, x2)\n",
    "#     HiC_map[x1,x2] += 1\n",
    "\n",
    "# method 2\n",
    "contacts = np.stack((all_x[:-1], all_x[1:]), axis=1)\n",
    "x1, x2 = contacts.T  # Décompose en deux tableaux\n",
    "HiC_map[x1, x2] += 1\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(HiC_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "matrix_x = [\n",
    "    [0,2,4,7,25,34,51,60,61,67,70],\n",
    "    [1,99],\n",
    "    [2,5,9,45,67,99],\n",
    "    [22,44,55,88]\n",
    "]\n",
    "\n",
    "Lmax= 100\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_x,marker='+')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "HiC_map = np.zeros((Lmax, Lmax))\n",
    "\n",
    "# # method 1\n",
    "# for x1, x2 in zip(all_x, all_x[1:]):\n",
    "#     print(x1, x2)\n",
    "#     HiC_map[x1,x2] += 1\n",
    "\n",
    "# method 2\n",
    "for array_x in matrix_x :\n",
    "    contacts = np.stack((array_x[:-1], array_x[1:]), axis=1)\n",
    "    x1, x2 = contacts.T\n",
    "    x1 = x1.astype(int)\n",
    "    x2 = x2.astype(int)\n",
    "    HiC_map[x1, x2] += 1\n",
    "\n",
    "HiC_map_plot = HiC_map / HiC_map.sum()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(HiC_map_plot, cmap='plasma', vmax=0.10)\n",
    "plt.colorbar(label=\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# En gros, est-ce que FACT ouvre sur un nombre de paires de base constante ou à un rythme constant ?\n",
    "# Je dirais plutot rythmr constant comme ça le taux traduit sa syamique elle-mêm reliée à son nombre de paires de base à ouvrir\n",
    "# Donc avant de faire le cas linaire on va juste faire le cas de alphao devient alphar\n",
    "# alphar pour alpha_remodelling\n",
    "\n",
    "\n",
    "# Sizes\n",
    "s = 150\n",
    "l = 10\n",
    "\n",
    "# Acceptance\n",
    "alphao = 0\n",
    "alphaf = 1\n",
    "\n",
    "# Occupation\n",
    "oo = 1 - alphao\n",
    "of = 1 - alphaf\n",
    "\n",
    "# Arrays\n",
    "oo_array = np.array([oo] * s)\n",
    "of_array = np.array([of] * l)\n",
    "\n",
    "# Nucleosomal unit\n",
    "NU = np.concatenate((of_array, oo_array, of_array))\n",
    "# print(NU)\n",
    "\n",
    "\n",
    "\n",
    "# Values\n",
    "a1 = int(s/2)\n",
    "a2 = int(s/2)\n",
    "b = s - (a1 + a2)\n",
    "\n",
    "# Profile : linear\n",
    "a1_array = np.linspace(0, 1, a1 + 1)\n",
    "a2_array = np.linspace(1, 0, a2 + 1)\n",
    "b_array = np.ones(b)\n",
    "linear_nuc = np.concatenate((of_array[:-1], a1_array, b_array, a2_array, of_array[-1:]))\n",
    "\n",
    "\n",
    "# Picturing\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(NU, label='Nucleosomal occupancy', c='k', lw=1)\n",
    "plt.plot(linear_nuc, label='New occupancy : linear', c='r', lw=1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remodelling_obstacle(obstacle: np.ndarray, alphar: np.float64):\n",
    "\n",
    "    new_obs = np.asarray(obstacle, dtype=np.float64)\n",
    "    new_obs[:] = alphar\n",
    "\n",
    "    return new_obs\n",
    "\n",
    "\n",
    "array = [1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1]\n",
    "print(array)\n",
    "alpha_new = 0.47\n",
    "new_array = remodelling_obstacle(obstacle=array, alphar=alpha_new)\n",
    "print(new_array)\n",
    "# Attention ici on ne prend pas un obstacle mais un paysage entier il nous faut donc une fonction de detection d'remodelling_obstacle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_obstacle(landscape: np.ndarray, alphaf:np.float64, alphao: np.float64):\n",
    "\n",
    "    obstacle = np.array([])\n",
    "\n",
    "    return obstacle\n",
    "\n",
    "obstacle_found = find_obstacle(landscape=array, alphaf=1, alphao=0)\n",
    "print(obstacle_found)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_obstacle_blocks(array, value=0.01):\n",
    "    array = np.asarray(array)\n",
    "    is_obstacle = np.isclose(array, value)\n",
    "    diff = np.diff(is_obstacle.astype(int))\n",
    "    starts = np.where(diff == 1)[0] + 1\n",
    "    ends = np.where(diff == -1)[0] + 1\n",
    "\n",
    "    # Cas où le début/fin est un obstacle\n",
    "    if is_obstacle[0]:\n",
    "        starts = np.insert(starts, 0, 0)\n",
    "    if is_obstacle[-1]:\n",
    "        ends = np.append(ends, len(array))\n",
    "\n",
    "    return list(zip(starts, ends))  # liste de tuples (start, end)\n",
    "\n",
    "# Exemple d’utilisation :\n",
    "array = np.array([1, 1, 0.01, 0.01, 0.01, 1, 0, 0.01, 0.01, 1])\n",
    "obstacle_blocks = find_obstacle_blocks(array)\n",
    "\n",
    "# Simuler un saut sur index i\n",
    "def remodel_if_needed(array, index, obstacle_blocks, new_val, p_remodel=0.5):\n",
    "    for start, end in obstacle_blocks:\n",
    "        if start <= index < end:\n",
    "            if np.random.rand() < p_remodel:\n",
    "                array[start:end] = new_val\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_python_3.11.5-GCCcore-13.2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
